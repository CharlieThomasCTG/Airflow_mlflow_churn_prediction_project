{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function\n",
    "import os\n",
    "import sys\n",
    "\n",
    "new_directory = \"E:/airflow/airflow\"\n",
    "current_directory = os.getcwd()\n",
    "scripts_path = os.path.abspath(os.path.join(os.getcwd(), '../scripts'))\n",
    "\n",
    "def change_directory(current_directory, new_directory,scripts_path):\n",
    "    # Get the current working directory\n",
    "    print(f'Current directory: {current_directory}')\n",
    "    # Define the path to change to\n",
    "    new_directory = \"E:/airflow/airflow\"\n",
    "    try:\n",
    "        # Change the current working directory\n",
    "        os.chdir(new_directory)\n",
    "        # Verify the change\n",
    "        current_directory = os.getcwd()\n",
    "        print(f'Current directory changed to: {current_directory}')\n",
    "    except FileNotFoundError:\n",
    "        print(f'Error: The directory \"{new_directory}\" does not exist.')\n",
    "    except PermissionError:\n",
    "        print(f'Error: Permission denied to change to \"{new_directory}\".')\n",
    "    except Exception as e:\n",
    "        print(f'An unexpected error occurred: {e}')\n",
    "    # Add the scripts directory to the Python path\n",
    "    sys.path.append(scripts_path)\n",
    "\n",
    "    \n",
    "change_directory(current_directory, new_directory, scripts_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "import sklearn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up all directory\n",
    "root_folder = new_directory\n",
    "database_path = root_folder+\"/database/\"\n",
    "data_directory = root_folder+\"/data/raw/\"\n",
    "data_profile_path = root_folder+\"/data/profile_report/\"\n",
    "intermediate_data_path = root_folder+\"/data/interim/\"\n",
    "final_processed_data_path = root_folder+\"/data/processed/\"\n",
    "\n",
    "old_data_directory = root_folder+\"/data/raw/\"\n",
    "new_data_directory = root_folder+\"/data/new/\"\n",
    "intermediate_path = root_folder+\"/data/interim/\"\n",
    "\n",
    "\n",
    "# Database\n",
    "db_path = root_folder+\"/database/\"\n",
    "db_file_name = \"feature_store_v01.db\"\n",
    "drfit_db_name = \"drift_db_name.db\"\n",
    "date_columns = ['registration_init_time','transaction_date_min','transaction_date_max','membership_expire_date_max','last_login']\n",
    "drift_db_name = \"drift_db_name.db\"\n",
    "\n",
    "# Mlflow\n",
    "mlflow_tracking_uri = \"http://Localhost:6006\"\n",
    "ml_flow_model_path = root_folder+ \"/mlruns/2/cb66e22bcbf74ded99dc219eb29e7609/artifacts/models/\"\n",
    "ml_flow_path = root_folder+ \"/mlruns/2/cb66e22bcbf74ded99dc219eb29e7609\"\n",
    "\n",
    "run_on = \"old\" #\"old\"\n",
    "append=False\n",
    "date_transformation = False\n",
    "start_date = '2017-03-01'\n",
    "end_date = '2017-03-31'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data=pd.read_csv(root_folder + '/data/data_unseen.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 270
    },
    "id": "X1fCPPbuyvCf",
    "outputId": "aeb2c80f-d6c1-443c-9819-b32a65d50bb1"
   },
   "outputs": [],
   "source": [
    "new_data.drop(['msno'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EF7oZgtr0vea"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def get_data_prepared_for_modeling(dataframe, scale_method='standard', date_columns=None, corr_threshold=0.90):\n",
    "    print(\"Initial columns:\", len(dataframe.columns))\n",
    "    \n",
    "    # Step 1: Date Conversion\n",
    "    if date_columns:\n",
    "        for column in date_columns:\n",
    "            try:\n",
    "                dataframe[column] = pd.to_datetime(dataframe[column], errors='coerce')\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {column}: {e}\")\n",
    "    \n",
    "    # Step 2: Remove Multicollinearity\n",
    "    # Create correlation matrix\n",
    "    corr_matrix = dataframe.corr().abs()\n",
    "    # Select upper triangle of correlation matrix\n",
    "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "    # Find features with correlation greater than threshold\n",
    "    to_drop = [column for column in upper.columns if any(upper[column] > corr_threshold)]\n",
    "    print(\"Columns to drop due to high correlation:\", to_drop)\n",
    "    dataframe.drop(to_drop, axis=1, inplace=True)\n",
    "    print(\"Columns after dropping:\", len(dataframe.columns))\n",
    "    \n",
    "    # Step 3: Feature Engineering on Date Columns\n",
    "    if date_columns:\n",
    "        features = [\"day\", \"month\", \"year\", \"weekday\"]\n",
    "        for eachcol in date_columns:\n",
    "            if eachcol in dataframe.columns:\n",
    "                # Ensure conversion\n",
    "                dataframe[eachcol] = pd.to_datetime(dataframe[eachcol], errors='coerce')\n",
    "                for eachfeature in features:\n",
    "                    col_name = f\"{eachcol}_{eachfeature}\"\n",
    "                    if eachfeature == 'day':\n",
    "                        dataframe[col_name] = dataframe[eachcol].dt.day\n",
    "                    elif eachfeature == 'month':\n",
    "                        dataframe[col_name] = dataframe[eachcol].dt.month\n",
    "                    elif eachfeature == 'year':\n",
    "                        dataframe[col_name] = dataframe[eachcol].dt.year\n",
    "                    elif eachfeature == 'weekday':\n",
    "                        dataframe[col_name] = dataframe[eachcol].dt.weekday\n",
    "        \n",
    "        # Drop original date columns\n",
    "        dataframe.drop(date_columns, axis=1, inplace=True)\n",
    "    \n",
    "    # Step 4: Scaling Numeric Features\n",
    "    column_to_scale = dataframe.select_dtypes(include=['float64', 'int64']).columns.drop('is_churn')\n",
    "    if scale_method == 'standard':\n",
    "        transformer = StandardScaler().fit(dataframe[column_to_scale])\n",
    "        scaled_data = pd.DataFrame(transformer.transform(dataframe[column_to_scale]), columns=column_to_scale)\n",
    "    \n",
    "    # Step 5: Combining Scaled and Other Features\n",
    "    final_df = pd.concat([scaled_data, dataframe.drop(column_to_scale, axis=1)], axis=1)\n",
    "    \n",
    "    # Step 6: Splitting X, y\n",
    "    X = final_df.drop(['is_churn'], axis=1)\n",
    "    y = final_df[['is_churn']]\n",
    "    \n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x7Z_0_ArkLWT",
    "outputId": "c16e3a0c-c5f7-4cf1-fc45-eb48d93b27a7"
   },
   "outputs": [],
   "source": [
    "X,y = get_data_prepared_for_modeling(new_data,date_columns = ['registration_init_time','transaction_date_min','transaction_date_max','membership_expire_date_max','last_login'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "import lightgbm as lgb\n",
    "\n",
    "def get_train_model(X,y):\n",
    "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\n",
    "  #Model Training\n",
    "  clf = lgb.LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
    "               device='gpu', importance_type='split', learning_rate=0.1,\n",
    "               max_depth=-1, min_child_samples=20, min_child_weight=0.001,\n",
    "               min_split_gain=0.0, n_estimators=100, n_jobs=-1, num_leaves=31,\n",
    "               objective=None, random_state=42, reg_alpha=0.0, reg_lambda=0.0,\n",
    "               subsample=1.0, subsample_for_bin=200000,\n",
    "               subsample_freq=0)\n",
    "  clf.fit(X_train, y_train)\n",
    "  # predict the results\n",
    "  y_pred=clf.predict(X_test)\n",
    "  # view accuracy\n",
    "  accuracy=accuracy_score(y_pred, y_test)\n",
    "  print('LightGBM Model accuracy score: {0:0.4f}'.format(accuracy_score(y_test, y_pred)))\n",
    "  print(classification_report(y_test, y_pred))\n",
    "  print(confusion_matrix(y_test, y_pred))\n",
    "  return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predict(model,x):\n",
    "  return model.predict_proba(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full Pipeline \n",
    "\n",
    "#new data coming in with Y values \n",
    "#X,y = get_data_prepared_for_modeling(new_data,date_columns = ['registration_init_time','transaction_date_min','transaction_date_max','membership_expire_date_max','last_login'])\n",
    "\n",
    "# Train the model with X & y \n",
    "model = get_train_model(X,y)\n",
    "\n",
    "#Predict Object \n",
    "predictions = get_predict(model,X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A5pC6c0MXInS",
    "outputId": "4d418213-be54-4efd-e0df-0fe2ab02d90a"
   },
   "outputs": [],
   "source": [
    "import lightgbm as lgbm\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix,classification_report\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "num_splits = 20\n",
    "strat_kf = StratifiedKFold(n_splits=num_splits, shuffle=True, random_state=42)\n",
    "\n",
    "scores = np.empty(num_splits)\n",
    "for idx, (train_idx, test_idx) in enumerate(strat_kf.split(X, y)):\n",
    "    print(\"=\" * 10 + f\"Training fold {idx}\" + 10 * \"=\")\n",
    "\n",
    "    X_train, X_val = X.iloc[train_idx], X.iloc[test_idx]\n",
    "    y_train, y_val = y.iloc[train_idx], y.iloc[test_idx]\n",
    "    eval_set = [(X_val, y_val)]\n",
    "\n",
    "    lgbm_clf = lgbm.LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
    "               importance_type='split', learning_rate=0.1,\n",
    "               max_depth=-1, min_child_samples=20, min_child_weight=0.001,\n",
    "               min_split_gain=0.0, n_estimators=100, n_jobs=-1, num_leaves=31,\n",
    "               objective=None, random_state=42, reg_alpha=0.0, reg_lambda=0.0,\n",
    "               subsample=1.0, subsample_for_bin=200000,\n",
    "               subsample_freq=0)\n",
    "    lgbm_clf.fit(X_train, y_train)\n",
    "    # predict the results\n",
    "    y_pred=lgbm_clf.predict(X_val)\n",
    "    # view accuracy\n",
    "    accuracy=accuracy_score(y_val, y_pred)\n",
    "    print('LightGBM Model accuracy score: {0:0.4f}'.format(accuracy))\n",
    "    print(classification_report(y_val, y_pred))\n",
    "    print(confusion_matrix(y_val, y_pred))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Model trainer_lgbm",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
